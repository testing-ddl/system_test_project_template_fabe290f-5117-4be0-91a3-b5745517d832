{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0999d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import ctranslate2\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef80195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7278c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ctranslate model. Please change the path below according to your project\n",
    "generator = ctranslate2.Generator(\"/mnt/artifacts/ct2_int8\", device=\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70295e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN ðŸ™Š Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting ðŸ˜± To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on ðŸ˜‚\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: ðŸ™‰\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan ðŸ˜‚\n",
      "Hilary: ðŸ˜Ž\n",
      "Elliot: See you at 2 then xx\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a random test sample\n",
    "# sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# Change the index to select a different sample\n",
    "sample = test_dataset[5]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91df0b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamin is tired and will take a nap at 2. Hilary is meeting the French people at 2 and then will go to the Italian restaurant. Elliot will meet at 2.  They're all going to take the keys and have a\n"
     ]
    }
   ],
   "source": [
    "# generate output from the ctranslate model and print how long it took\n",
    "start_time = time.perf_counter()\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(test_sample)) \n",
    "max_length = 50\n",
    "results = generator.generate_batch([tokens], sampling_topk=1, max_length=max_length, include_prompt_in_result=False)\n",
    "output = tokenizer.decode(results[0].sequences_ids[0])\n",
    "end_time = time.perf_counter()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf52906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ctranslate model took 1.196 sec and generated 41.794 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n ctranslate model took {round(end_time - start_time, 3)} sec and generated {round(max_length / (end_time - start_time),3)} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d140b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "results = generator.generate_batch([tokens], sampling_topk=1, max_length=max_length, include_prompt_in_result=True)\n",
    "output = tokenizer.decode(results[0].sequences_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
