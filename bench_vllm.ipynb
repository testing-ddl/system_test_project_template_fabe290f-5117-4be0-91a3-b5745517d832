{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25869605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model to vLLM. Please change the path below according to your project\n",
    "llm = LLM(model=\"/mnt/artifacts/falcon_7b_model_adapter\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random test sample\n",
    "# sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# Change the index to select a different sample\n",
    "sample = test_dataset[5]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "start_time = time.perf_counter()\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "end_time = time.perf_counter()\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(f\"{generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427142b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n vLLM took {round(end_time - start_time, 3)} sec and generated {round(max_tokens / (end_time - start_time),3)}  tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f521917",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705aafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
